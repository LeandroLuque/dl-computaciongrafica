{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP Final DL-CG",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E-fmM_UQLI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install parse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GBJOSvKot16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "import zipfile\n",
        "from parse import *\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Torchvision\n",
        "import torchvision.utils\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCQXU6N-T5vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLUqVf45oN7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Configuraciones globales\n",
        "\n",
        "# Numero de canales de entrada (3= RGB)\n",
        "nc = 3\n",
        "# Tamaño del latent vector (z)\n",
        "latent_dims = 512\n",
        "# Learning rate\n",
        "lr = 1e-3\n",
        "# Cantidad de épocas de entrenamiento\n",
        "num_epochs = 10\n",
        "# Tamaño del batch\n",
        "batch_size = 128\n",
        "# Escala de la imagen\n",
        "image_size = 64\n",
        "capacity = 32\n",
        "# Cantidad de GPUs disponibles\n",
        "ngpu = 1\n",
        "# Directorio de descarga del dataset\n",
        "dataroot = \"datasets/shapenet/\"\n",
        "\n",
        "rs = np.random.RandomState(123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vumxczMH5do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Usar esta opción si está en colab\n",
        "\n",
        "#!wget https://www.dropbox.com/s/lc01fm5o8dbrp59/nvs_chair.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXwtY0xAne2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with zipfile.ZipFile(\"nvs_chair.zip\",\"r\") as zip_ref:\n",
        "#  zip_ref.extractall(dataroot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU0HNLARtrk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Manipulación y preparación de los datos.\n",
        "\n",
        "class ImgDataset(Dataset):\n",
        "\n",
        "  def __init__(self, ids):\n",
        "    \"\"\"\n",
        "      Modela el dataset\n",
        "    \"\"\"\n",
        "    self._ids = list(ids)\n",
        "    self.dataset_name = \"chair\"\n",
        "    self.img_path = 'datasets/shapenet'\n",
        "    self.transform = transforms.Compose([\n",
        "              transforms.Resize(image_size),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize((0.5,), (0.5,), (0.5,))\n",
        "            ])\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "      Para una imágen en particular de un objeto se seleccion una imágen \n",
        "      del mismo objeto con un viewpoint distinto. Por lo tanto, este método\n",
        "      devuelve la 3-upla (imagen, target_image, target_viewpoint)\n",
        "    \"\"\"\n",
        "    img_id = self._ids[idx]\n",
        "    only_id = img_id.split(\"_\")[0]\n",
        "    \n",
        "    src_img = self.readImageToArray(img_id)\n",
        "\n",
        "    elevation = random.choice([0, 10, 20])\n",
        "    rotation = random.choice(range(0, 36, 2))\n",
        "\n",
        "    rsin, rcos = math.sin(math.radians(rotation*10)), math.cos(math.radians(rotation*10))\n",
        "    esin, ecos = math.sin(math.radians(elevation)), math.cos(math.radians(elevation))\n",
        "\n",
        "\n",
        "    dst_img = self.readImageToArray(\"{}_{}_{}\".format(only_id, rotation,elevation))\n",
        "\n",
        "    src_img = self.transform(src_img)\n",
        "    dst_img = self.transform(dst_img)\n",
        "\n",
        "    return (src_img, dst_img, torch.tensor([rsin, rcos, esin, ecos]).float())\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self._ids)\n",
        "\n",
        "\n",
        "  def readImageToArray(self, in_id):\n",
        "    img = Image.open(self.img_path + '/' + self.dataset_name + '/' + in_id + '.png')\n",
        "    return img\n",
        "\n",
        "\n",
        "def all_ids(dataset_name='chair', shuffle_train=True, shuffle_test=True):\n",
        "    \"\"\"\n",
        "      Tomado del repo de TB-Networks.\n",
        "      Usa los archivos de train y validación para armar los arreglos donde \n",
        "      se emplean los códigos que identifican a cada objeto.\n",
        "    \"\"\"\n",
        "    import os.path as osp\n",
        "\n",
        "    with open(osp.join('.', 'id_' + dataset_name + '_train.txt'), 'r') as fp:\n",
        "        ids_train = [s.strip() for s in fp.readlines() if s]\n",
        "    if shuffle_train:\n",
        "        rs.shuffle(ids_train)\n",
        "\n",
        "    with open(osp.join('.', 'id_' + dataset_name + '_test.txt'), 'r') as fp:\n",
        "        ids_test = [s.strip() for s in fp.readlines() if s]\n",
        "    if shuffle_test:\n",
        "        rs.shuffle(ids_test)\n",
        "\n",
        "    return ids_train, ids_test\n",
        "\n",
        "\n",
        "ids_train, ids_test = all_ids()\n",
        "\n",
        "dataset_train = ImgDataset(ids_train)\n",
        "\n",
        "train_loader = DataLoader(dataset_train,\n",
        "                          batch_size=128,\n",
        "                          num_workers=3, drop_last=True,\n",
        "                          shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LbYwSgX3xzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAMU_HyLAOW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_imgs, dst_imgs, viewpoint = next(iter(train_loader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(src_imgs.to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MyISRpLAnir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcooIsdvoW5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDNR6oy8rrsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Arquitetura de Red 8192"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryvmbn9jsWAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.conv1 = nn.Conv2d(in_channels=nc, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14\n",
        "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=c*2, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7\n",
        "        \n",
        "        self.fc = nn.Linear(in_features=c*2*8*8, out_features=latent_dims)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)     \n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        c = capacity\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=latent_dims+64, out_features=latent_dims)\n",
        "        self.fc2 = nn.Linear(in_features=latent_dims, out_features=latent_dims)\n",
        "        self.fc3 = nn.Linear(in_features=latent_dims, out_features=c*2*8*8)\n",
        "\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.ConvTranspose2d(in_channels=c, out_channels=nc, kernel_size=4, stride=2, padding=1)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = x.view(x.size(0), capacity*2, 8, 8)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.tanh(self.conv3(x)) \n",
        "        return x\n",
        "\n",
        "class Viewpoint(nn.Module):\n",
        "    \"\"\"\n",
        "      Parte de la red que modela el viewpoint de una imagen.\n",
        "      El viewpoint de la imágen está formado por el ángulo de rotación\n",
        "      y el ángulo de elevación.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "      super(Viewpoint, self).__init__()\n",
        "      self.fc1 = nn.Linear(in_features=4, out_features=64)\n",
        "      self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
        "      self.fc3 = nn.Linear(in_features=64, out_features=64)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = F.relu(self.fc3(x))\n",
        "      return x\n",
        "    \n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.viewpoint = Viewpoint()\n",
        "\n",
        "    def forward(self, x, theta):\n",
        "        \"\"\"\n",
        "          Para evitar confusión, theta no es una ángulo\n",
        "          particular sino que es es el vector de viewpoint.\n",
        "        \"\"\"\n",
        "        latent = self.encoder(x)\n",
        "        theta = self.viewpoint(theta)\n",
        "        result = torch.cat((latent, theta), dim=1)\n",
        "        x_recon = self.decoder(result)\n",
        "        return x_recon\n",
        "    \n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "autoencoder = autoencoder.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBiTmvbkva6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % num_params)\n",
        "print(autoencoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ESBvwGHUGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Tomdas del repo de ejemplo\n",
        "# Funciones de graficación de las imagenes\n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = img.cpu()\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, viewpoint, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        images = images.to(device)\n",
        "        viewpoint = viewpoint.to(device)\n",
        "        images = model(images, viewpoint)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPOxHCx57wI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(params=autoencoder.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "# red en modo entrenamiento\n",
        "autoencoder.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "\n",
        "    image_batch_recon = None\n",
        "    viewpoint = None\n",
        "    \n",
        "    #for image_batch,_,rotation,elevation in dataloader:\n",
        "    for src_imgs, dst_imgs, viewpoint in train_loader:\n",
        "        \n",
        "        #image_batch = image_batch.to(device)\n",
        "        src_imgs = src_imgs.to(device)\n",
        "        dst_imgs = dst_imgs.to(device)\n",
        "        viewpoint = viewpoint.to(device)\n",
        "       \n",
        "        # reconstrucción del autoencoder\n",
        "        image_batch_recon = autoencoder(src_imgs, viewpoint)\n",
        "        \n",
        "        # error de reconstrucción\n",
        "        loss = F.mse_loss(image_batch_recon, dst_imgs)\n",
        "        \n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # optimizamos los pesos usando el gradiente propagado por backprop\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    print(\"Input\")\n",
        "    show_image(torchvision.utils.make_grid(src_imgs[:50],10,5))\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Target\")\n",
        "    show_image(torchvision.utils.make_grid(dst_imgs[:50],10,5))\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Reconstruction\")\n",
        "    visualise_output(image_batch_recon, viewpoint, autoencoder)\n",
        "        \n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))\n",
        "    print(\"-\"*25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yBBzRF2VdWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLJHEeCO8ULB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Reconstruction error')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBnN4thM8lPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKRIE0ki-Wwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Model en mode evaluación.\n",
        "autoencoder.eval()\n",
        "\n",
        "#iterator = iter(dataloader)\n",
        "iterator = iter(train_loader)\n",
        "iterator.next()\n",
        "iterator.next()\n",
        "\n",
        "images, target, viewpoint = iterator.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL4jfGkmn7wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D85NBtJc6a95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Generamos una UI minima para interectuar con la red y generar \n",
        "### las nuevas vistas."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIsooaOE6aui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBWrslfzsFmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.eval()\n",
        "\n",
        "def f(rotation, elevation):\n",
        "  with torch.no_grad():\n",
        "\n",
        "    rsin, rcos = math.sin(math.radians(rotation)), math.cos(math.radians(rotation))\n",
        "    esin, ecos = math.sin(math.radians(elevation)), math.cos(math.radians(elevation))\n",
        "\n",
        "    viewpoint = [rsin, rcos, esin, ecos]\n",
        "\n",
        "    viewpoint = torch.tensor([viewpoint])\n",
        "    viewpoint = viewpoint.repeat(batch_size, 1).float()\n",
        "    viewpoint = viewpoint.to(device)\n",
        "\n",
        "    batch = images.to(device)\n",
        "\n",
        "    resultado = autoencoder(batch, viewpoint)\n",
        "\n",
        "    resultado = resultado.cpu()\n",
        "    resultado = to_img(resultado)\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    np_imagegrid = torchvision.utils.make_grid(resultado[:50], 10, 5).numpy()\n",
        "    plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "interactive_plot = interactive(f, rotation=widgets.IntSlider(min=0, max=360, step=1, value=10),\\\n",
        "                               elevation=widgets.IntSlider(min=0, max=20, step=1, value=10))\n",
        "output = interactive_plot.children[-1]\n",
        "output.layout.height = '350px'\n",
        "interactive_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acBZqrh7tlo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwoh0fzA4z7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}